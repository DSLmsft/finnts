---
title: "Parallel Processing"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{parallel-processing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Local Machine

When "parallel_processing" input within "forecast_time_series" is set to the "local_machine", each time series (including training models on the entire data set) is ran in parallel on the users local machine. Each time series will run on a separate core of the machine. Hyperparameter tuning, model refitting, and model averaging will be ran sequentially, which cannot be done in parallel since a parallel process is already running on the machine for each time series. This works well for data that contains many time series where you might only want to run a few simpler models, and in scenarios where cloud computing is not available. 

If "parallel_processing" is set to "none" and "run_model_parallel" is set to "TRUE" within the "forecast_time_series", then each time series is ran sequentially but the hyperparameter tuning, model refitting, and model averaging is ran in parallel. This works great for data that has a limited number of time series where you want to run a lot of back testing and build dozens of models within Finn. 

Lastly, if "parallel_processing" is set to "none" and "run_model_parallel" is set to "FALSE", then everything will run sequentially. This is definitely the slowest way to build forecasts with Finn, but could be the best choice if no cloud options are available and your local machine may not have many cores or large RAM. 

## Within Azure using Azure Batch

To leverage the full power of Finn, running within Azure is the best choice in building production ready forecasts that can easily scale. The most efficient way to run Finn is to set "parallel_processing" to "azure_batch" and set "run_model_parallel" to "TRUE". This will run each time series in separate virtual machines (VM) in Azure. Within each VM, hyperparameter tuning, modeling refitting, and model averaging are all done in parallel across the cores available on the machine (or defined by the user). 

[Azure Batch](https://azure.microsoft.com/en-us/services/batch/#overview) is a powerful resource from Microsoft Azure, that allows for easily salable parallel compute. Finn leverages the [doAzureParallel](https://github.com/Azure/doAzureParallel) and rAzureBatch packages built by Microsoft to connect to Azure batch. Refer to their [github site](https://github.com/Azure/doAzureParallel) for more information about how it works under the hood and how to set up your own Azure Batch resource to use with Finn. 

Finn makes it easy to connect to Azure Batch by providing the "azure_batch_credentials" and "azure_batch_cluster_config" inputs within the "forecast_time_series" function. This allows you to easily input the Azure account/compute information needed to either build and deploy a new compute cluster or connect to an existing one. Please refer to their github to learn more about the specifics. 

At a high level, all you have to do is pass something similar to what you see below, while making sure to input your specific azure account information.

```{r}
azure_batch_credentials <- list(
  "sharedKey" = list(
    "batchAccount" = list(
      "name" = "<insert resource name>",
      "key" = "<insert compute key>",
      "url" = "<insert resource URL>"
    ),
    "storageAccount" = list(
      "name" = "<insert resource name>",
      "key" = "<insert compute key>",
      "endpointSuffix" = "core.windows.net"
    )
  ),
  "githubAuthenticationToken" = "<insert github PAT>",
  "dockerAuthentication" = list("username" = "",
                                "password" = "",
                                "registry" = "")
)

azure_batch_cluster_config <- list(
  "name" = "<insert compute cluster name>",
  "vmSize" = "Standard_D5_v2", # solid VM size that has worked well in the past with Finn forecasts
  "maxTasksPerNode" = 1, # tells the cluster to only run one unique time series for each VM. That enables us to then run another layer of parallel processing within the VM
  "poolSize" = list(
    "dedicatedNodes" = list(
      "min" = 1,
      "max" = 200
    ),
    "lowPriorityNodes" = list(
      "min" = 1,
      "max" = 100
    ),
    "autoscaleFormula" = "QUEUE" # automatically scales up VM's as more jobs get sent to the cluster. 
  ),
  "containerImage" = "mftokic/finn-azure-batch-dev", # docker image you can use that automatically downloads software needed for Finn to run in cloud
  "rPackages" = list(
    "cran" = c('Rcpp', 'modeltime', 'modeltime.ensemble', 'modeltime.resample', 'tidymodels', 'lubridate', 'rlist', 'rules', 'Cubist', 'earth', 'kernlab', 'doParallel', 'tidyverse', 'lightgbm', 'torch', 'tabnet', 'prophet', 'gtools'), # finnts package dependencies
    "github" = list(),
    "bioconductor" = list()
  ),
  "commandLine" = list()
)


```

Passing these values into "forecast_time_series" will ensure that finnts correctly runs within Azure Batch. The best part of Azure Batch is how it can easily scare to more compute as needed. In the above example, the lowest amount of VM's running at any time will be 2, and can easily scale to 300 when needed. This allows you to pay for extra compute only when you need it, and allows for forecasts to run that much quicker. You can have separate Finn forecasts (different data sets or inputs) submitted to the same Azure Batch cluster to all run in parallel. How cool is that?!


